 ARTIFICAL NEURAL NETWORK
The basics behind artificial neural networks are to model a highly abstracted version of a complex neural biological network by mapping out a structure of interconnected group of neurons found in the brain.   An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information.   ARCHITECTURE OF NNs
The two basic types of NNs are those that have to be trained (e.  g. , the backpropagation method that is used here) and those that learn or organize on their own.   Each neurode combines a number of inputs and produces an output, which is transmitted to many different locations, including other neurodes.   It is composed of a large number of highly interconnected processing elements (neurons)[1] working in unison to solve specific problems.   DREF_IMG4.1
 :
A simple neural network
DREF_IMG4.1 shows a very basic structure of an ANN.   Inputs to the ANN are usually binary, either"  0 and 1"  or" -1 and +1.  REF_IMG 2.1 Learning in biological systems involves adjustments to the synaptic connections that exist between the neurons.   These are usual feed-forward artificial networks where the data is passed through each layer until it reaches the output nodes of the network.  REF_IMG 4.3 This value is then compared to an 
activation threshold
 specified for that network or node alone.   Neural networks resemble the human brain in the following two ways: they acquire knowledge through learning, and the knowledge is stored within inter-neuron connection strengths known as synaptic weights[1,12].   The higher the value of a weight on a link, the stronger the effect is on the input and the higher the effect is towards the threshold level. .   When there is a large data set of input and outputs available for some system, a supervised training procedure can be used to create an ANN.   DREF_IMG4.2 : 
input summation through activation function
DREF_IMG4.2 is an overview of how the inputs are summated through to this 
activation function
If we require non-distinct outputs we can simply map the outputs to a logistic sigmoid function.   Adjusting these values by hand is feasible but when networks become overly complex it is better to solve this problem to using algorithms that will adjust the weights accordingly.  REF_IMG 4.3 These networks are fine-grained parallel implementations of nonlinear static or dynamic systems.   DREF_IMG1.1: Diagram of 4-layer Perceptron with two hidden layers 
The inputs are fed into the input layer and get multiplied by interconnection weights as they are passed from the input layer to the first hidden layer.   According to Maier and Dandy (1997), ANNs were first introduced to the water resource community by Daniell (1991), who used ANNs to predict monthly water consumption and to estimate flood occurrence.   Machine Learning Algorithms
In a particular network, there are various techniques available:
Supervised 
 This type of learning paradigm, in particular, manipulates the ANN weights allowing network outputs to match that of an example output, or a 
training set.  REF_IMG 4.3 A very important feature of these networks is their adaptive nature where"  learning by example"  replaces"  programming"  in solving problems.   These include time-series prediction for rainfall forecasting (French, 1992), reservoir inflow time series (Raman and Sunilkumar, 1995), rainfall-runoff processes (Hsu, 1995 and Shamseldin, 1997), and river salinity (Maier and Dandy, 1996).   This feature makes such computational models very appealing in application domains where one has little or incomplete understanding of the problem to be solved, but where training data is available.   ANNs also have been used for representing soil and water processes including soil moisture (Altenford, 1992), groundwater cleanup strategies (Ranjithan, 1993), water table fluctuations (Shukla, 1996; Yang, 1996a), pesticide movement in soils (Yang, 1996b), drainage pattern determination from a Digital Elevation Model (Kao, 1996), and water table management (Yang , 1998).   Finally the data is multiplied by interconnection weights then processed one last time within the output layer to produce the neural network output.  REF_IMG 1.1 Another key feature is the intrinsic parallel architecture which allows for fast computation of solutions when these networks are implemented on parallel digital computers or, ultimately, when implemented in customized hardware.   A set 
example values 
are compared to the real output values of a network.   Neural networks, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques.  REF_IMG 1.1 As an emerging field of modeling, many of the methods and techniques used in ANNs are not standardized and vary between texts and software applications.   This difference is called the 
error value.   Caudill and Butler (1992a) suggest the use of a trial and error approach for determining parameters and network architecture for a given problem.   We shall study Figure 1 in greater detail in a moment, for now; consider a brief introduction to the NN development process. .   These incude pattern classification, speech synthesis and recognition, adaptive interfaces between humans and complex physical systems, function approximation, image compression, associative memory, clustering, forecasting and prediction, combinatorial optimization, nonlinear system modeling, and control.   This expert can then be used to provide projections given new situations of interest and answer"  what if"  questions.   Still, some general guidelines emerge from the literature.   For each hidden layer, an error value is calculated for each neuron by comparing initial output error value and the updated weight values going out of the current neuron.  REF_IMG 4.3 Self-Organization: An ANN can create its own organization or representation of the information it receives during learning time.   In fact, the majority of the networks covered in this book are more closely related to traditional mathematical and/or statistical models such as non-parametric pattern classifiers, clustering algorithms, nonlinear filters, and statistical regression models than they do with neurobiological models. .  REF_IMG 3.1 Fault Tolerance via Redundant Information Coding: Partial destruction of a network leads to the corresponding degradation of performance.   DREF_IMG3.1 : MULTILAYER PERCEPTRON NEURAL NETWOR  One of the main limitations to this approach is that it has the tendency to get trapped in its local optima.  