<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Metadata starts -->
<meta name="author" content="ashish ">
<meta name="DC.creator" content="ashish ">
<meta name="revised" content="2011-11-09T16:25:51">
<meta name="DC.date" content="2011-11-09T16:25:51">
<meta name="generator" content="LibreOffice/3.3$Linux LibreOffice_project/330m19$Build-202">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<!-- Metadata ends -->
<style type="text/css">
  /* ODF paragraphs, by default, don't have any line spacing. */
  p { margin: 0px; padding: 0px; }
  /* put a default link style in, so we can see links */
  a[href] { color: blue; text-decoration: underline; }
  
 /* Document styles start */
  .default_graphic{
font-size: 12pt;
}p{
font-family: 'Times New Roman';
font-size: 12pt;
}table{
}tr{
}.Standard{
}.Heading{
margin-top: 0.423cm;
margin-bottom: 0.212cm;
font-family: 'Arial';
font-size: 14pt;
}.Text_20_body{
margin-top: 0cm;
margin-bottom: 0.212cm;
}.List{
margin-top: 0cm;
margin-bottom: 0.212cm;
}.Caption{
margin-top: 0.212cm;
margin-bottom: 0.212cm;
font-size: 12pt;
font-style: italic;
}.Index{
}.Heading_20_2{
margin-top: 0.423cm;
margin-bottom: 0.212cm;
font-family: 'Arial';
font-size: 14pt;
font-family: 'Times New Roman';
font-size: 18pt;
font-weight: bold;
}.Table_20_Contents{
}.Table_20_Heading{
text-align: center;
font-weight: bold;
}.Normal_20__28_Web_29_{
margin-top: 0.494cm;
margin-bottom: 0.494cm;
}.Article_20_Heading_20_2{
margin-top: 0cm;
margin-bottom: 0.212cm;
font-family: 'Book Antiqua';
font-size: 14pt;
font-weight: bold;
}.Frame_20_contents{
margin-top: 0cm;
margin-bottom: 0.212cm;
}.Heading_20_3{
margin-top: 0.423cm;
margin-bottom: 0.106cm;
font-size: 14pt;
font-weight: bold;
}.Emphasis{
font-style: italic;
}.Internet_20_link{
color: #000080;
text-decoration: underline;}.Strong_20_Emphasis{
font-weight: bold;
}.WW8Num2z0{
font-family: 'Symbol';
}.WW8Num16z0{
font-family: 'Wingdings';
}.WW8Num16z1{
font-family: 'Courier New';
}.WW8Num16z3{
font-family: 'Symbol';
}.Graphics{

    /* Centered alignment */
    margin-left: auto; margin-right: auto;
}.Frame{

    /* Centered alignment */
    margin-left: auto; margin-right: auto;
}.WW8Num2_1{ list-style-type: disc;}
.WW8Num2_2{ list-style-type: decimal;}
.WW8Num2_3{ list-style-type: decimal;}
.WW8Num2_4{ list-style-type: decimal;}
.WW8Num2_5{ list-style-type: decimal;}
.WW8Num2_6{ list-style-type: decimal;}
.WW8Num2_7{ list-style-type: decimal;}
.WW8Num2_8{ list-style-type: decimal;}
.WW8Num2_9{ list-style-type: decimal;}
.WW8Num2_10{ list-style-type: decimal;}
.WW8Num16_1{ list-style-type: disc;}
.WW8Num16_2{ list-style-type: circle;}
.WW8Num16_3{ list-style-type: square;}
.WW8Num16_4{ list-style-type: disc;}
.WW8Num16_5{ list-style-type: circle;}
.WW8Num16_6{ list-style-type: square;}
.WW8Num16_7{ list-style-type: disc;}
.WW8Num16_8{ list-style-type: circle;}
.WW8Num16_9{ list-style-type: square;}
.WW8Num16_10{ list-style-type: decimal;}

 /* Document styles end */

 /* Automatic styles start */
  .P1{
}.P2{
text-align: center;
}.P3{
}.P4{
}.P5{
font-weight: bold;
}.T1{
font-weight: bold;
}.T2{
font-weight: bold;
}.T3{
font-style: italic;
}.T4{
font-style: italic;
}.T5{
}.fr1{

    /* Centered alignment */
    margin-left: auto; margin-right: auto;
margin-left: 0cm;
margin-right: 0cm;
padding-left: 0.28cm;
padding-right: 0.28cm;
padding-top: 0.153cm;
padding-bottom: 0.153cm;
border: none;
}
 /* Automatic styles end */
</style>
</head>
<body>
<p class="P5">ARTIFICAL NEURAL NETWORK</p>
<p class="Standard">
<br>
</p>
<p class="Standard">The basics behind artificial neural networks are to model a highly abstracted version of a complex neural biological network by mapping out a structure of interconnected group of neurons found in the brain. All signals that run through these neurons are processed in parallel, thus involving multiple calculations at one particular time.</p>
<p class="P1">
<br>
</p>
<p class="P1">In an ANN each connection in the network is assigned a weight which determines the activity of the neural network.</p>
<p class="P1">
<br>
</p>
<p class="P2">
<div class="fr1" style="
      
      border: 1px solid #888;
      
        width: 7.172cm;
      
        height: 7.35cm;
      ">
<img style="
      
      width: 100%;
      height: 100%;
      
        display: block;
      " alt="" src="Pictures/10000000000001910000019B412453FC.png"></div>
</p>
<p class="P2">
<span class="T2">Diagram 1</span><span class="T1"> :</span><span class="T1">A simple neural network</span>
</p>
<p class="Standard">
<br>
</p>
<p class="Standard">Diagram 1 shows a very basic structure of an ANN. Each node (coloured circles) is connected to each other nodes through an artificial link, a synapse, that carries a certain value called a <span class="T3">weight</span>. This structure works from left to right, corresponding to the input, the hidden and the output layers of the network. These are usual feed-forward artificial networks where the data is passed through each layer until it reaches the output nodes of the network.</p>
<p class="Standard">
<br>
</p>
<p class="Standard">Weights that lay on the links between these nodes allow the data to be transferred to other connecting nodes. When these data enter a particular node as inputs, they are multiplied by their respective weights. This is done to all inputs going into one node. In a particular node, these weights are then added together to produce an <span class="T3">activation value</span>. This value is then compared to an <span class="T3">activation threshold</span> specified for that network or node alone. If the threshold is met then the neuron fires an output to other connected nodes within the network. In a <span class="T3">step function</span>, when the threshold is met the neuron/node will fire the output of 1, and if not then 0 will be the output value.  The higher the value of a weight on a link, the stronger the effect is on the input and the higher the effect is towards the threshold level.</p>
<p class="P2">
<div class="fr1" style="
      
      border: 1px solid #888;
      
        width: 10.848cm;
      
        height: 5.981cm;
      ">
<img style="
      
      width: 100%;
      height: 100%;
      
        display: block;
      " alt="" src="Pictures/100000000000019A000001037C9DA814.png"></div>
</p>
<p class="P2">
<span class="T1">Diagram 2 : </span><span class="T1">input summation through activation function</span>
</p>
<p class="P2">
<br>
</p>
<p class="Standard">Diagram 2 is an overview of how the inputs are summated through to this <span class="T3">activation function</span>.</p>
<p class="Standard">
<br>
</p>
<p class="Standard">If we require non-distinct outputs we can simply map the outputs to a logistic sigmoid function. Diagram 3 shows a graded output in relation to the activation values.</p>
<p class="Standard">
<br>
</p>
<p class="P2">
<div class="fr1" style="
      
      border: 1px solid #888;
      
        width: 7.938cm;
      
        height: 4.653cm;
      ">
<img style="
      
      width: 100%;
      height: 100%;
      
        display: block;
      " alt="" src="Pictures/100000000000012C000000F0823F79E9.png"></div>
</p>
<p class="P2">
<span class="T1">Diagram 3 :</span><span class="T1">graded outpu</span><span class="T1">t</span><span class="T1"> in ralation to the acitvation value</span>
</p>
<p class="Standard">
<br>
</p>
<p class="Standard">These weights can be modified to create desired outputs from our inputs in the artificial network. Adjusting these values by hand is feasible but when networks become overly complex it is better to solve this problem to using algorithms that will adjust the weights accordingly. Adjusting these weights relates to the term &ldquo;learning&rdquo; or &ldquo;training&rdquo; whereby the networks are allowed to formulate a solution to minimise the error amount between real outputs and expected outputs of a network, which is also an iterative process.</p>
<h3 class="Heading_20_3">
<a name="N65895"></a><a name="__RefHeading__663_899997840"><span style="font-size: 0px"> </span></a>Machine Learning Algorithms</h3>
<p class="P1">In a particular network, there are various techniques available:</p>
<ul class="WW8Num16_1">
<li>
<p class="P3">
<span class="T5">Supervised &ndash; This type of learning paradigm, in particular, manipulates the ANN weights allowing network outputs to match that of an example output, or a </span><span class="T4">training set</span><span class="T5">. The error value between the two data affects the weight mapping in the network on the next step.  (</span>Kotsiantis, S. 2007)</p>
</li>
<li>
<p class="P4">Unsupervised &ndash; This type of approach allows a network to run without any set of example outputs. </p>
</li>
<li>
<p class="P3">
<span class="T5">Reinforcement &ndash; The basic principle here is to allow a network to generate its own input data. This data is generated when the neural network is made to interact with its environment. (</span>Kaelbling et al. 1996)</p>
</li>
</ul>
<p class="P1">
<br>
</p>
<h3 class="Heading_20_3">
<a name="N65937"></a><a name="__RefHeading__665_899997840"><span style="font-size: 0px"> </span></a>BackPropagation</h3>
<p class="Standard">
<span class="T5">Backpropagation algorithm is a form of supervised learning used on feed-forward networks. A set </span><span class="T4">example values </span><span class="T5">are compared to the real output values of a network. This difference is called the </span><span class="T4">error value</span><span class="T5">. This error value is first used in adjusting the weights on the links connecting to the output neurons. The weights on links going into the output are usually on synapse from the hidden layers of the neural network. </span>
</p>
<p class="P1">
<br>
</p>
<p class="Standard">
<span class="T5">For each hidden layer, an error value is calculated for each neuron by comparing initial output error value and the updated weight values going out of the current neuron. This error value is then used to manipulate the weights going into the hidden layer neurons assuming that this network contains a single hidden layer.  The adjustments of weights works backwards to the input layer hence back propagation. When adjusting the weights, a </span><span class="T4">learning rate</span><span class="T5"> value is incorporated into the algorithm. This value simply effects how much the weight is adjusted, the higher the rate the bigger adjustments.</span>
</p>
<p class="P1">
<br>
</p>
<p class="P1">This cycle iterates until the error amount is within acceptable limits. One of the main limitations to this approach is that it has the tendency to get trapped in its local optima.</p>
</body>
</html>
